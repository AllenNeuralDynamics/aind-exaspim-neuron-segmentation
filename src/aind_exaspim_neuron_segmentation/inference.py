"""
Created on Wed June 28 12:00:00 2025

@author: Anna Grim
@email: anna.grim@alleninstitute.org

This Python file implements a 3D image processing pipeline for neuron
segmentation. It includes functions to run a neural network model to predict
either voxel affinities or foreground-background maps, generate segmentations
using agglomerative watershed, and optionally skeletonizes the result.

"""

from tqdm import tqdm

import itertools
import kimimaro
import numpy as np
import torch
import waterz
import zipfile

from aind_exaspim_neuron_segmentation.machine_learning.unet3d import UNet3D
from aind_exaspim_neuron_segmentation.utils import img_util


# --- Model Predictions ---
def predict(
    img,
    model,
    affinity_mode=True,
    batch_size=16,
    brightness_clip=1000,
    normalization_percentiles=(1, 99.9),
    patch_shape=(96, 96, 96),
    overlap=(32, 32, 32),
    trim=8,
    verbose=True
):
    """
    Predicts affinities or foreground–background maps for a 3D image by
    splitting it into overlapping patches, batching the patches, and
    processing each batch with the model.

    Parameters
    ----------
    img : numpy.ndarray
        Input 3D image with shape (1, 1, D, H, W).
    model : torch.nn.Module
        PyTorch model used for prediction.
    affinity_mode : bool, optional
        If True, the model predicts affinities; if False, it predicts
        foreground–background. Default is True.
    brightness_clip : float, optional
        Maximum brightness value for voxel intensities. Default is 1000.
    batch_size : int, optional
        Number of patches to process in a batch. Default is 32.
    normalization_percentiles : Tuple[int], optional
        Lower and upper percentiles used for normalization. Default is
        (1, 99.9).
    overlap : Tuple[int], optional
        Shape of overlap between patches along each dimension. Default is
        (16, 16, 16).
    patch_shape : Tuple[int], optional
        Shape of 3D patch expected by the model. Default is (128, 128, 128).
    trim : int, optional
        Number of voxels to trim from the edges of each patch in the output.
        Default is 8.
    verbose : bool, optional
        Indication of whether to show a tqdm progress bar. Default is True.

    Returns
    -------
    pred : numpy.ndarray
        Prediction generated by the given model applied to an image.
    """
    # Preprocess image
    img = np.minimum(img, brightness_clip)
    img = img_util.normalize(img, percentiles=normalization_percentiles)
    while len(img.shape) < 5:
        img = img[np.newaxis, ...]

    # Generate patch starts from a sliding window
    n_patches = count_patches(img.shape, patch_shape, overlap)
    starts_generator = generate_patch_starts(img.shape, patch_shape, overlap)
    pbar = tqdm(total=n_patches, desc="Predict") if verbose else None

    # Run model to generate prediction
    n_channels = 3 if affinity_mode else 1
    accum_pred = np.zeros((n_channels,) + img.shape[2:], dtype=np.float32)
    accum_wgt = np.zeros(img.shape[2:], dtype=np.float16)
    for _ in range(0, n_patches, batch_size):
        # Extract batch and run model
        starts = list(itertools.islice(starts_generator, batch_size))
        patches = _predict_batch(img, model, starts, patch_shape, trim=trim)

        # Add batch predictions to result
        for patch, start in zip(patches, starts):
            # Compute start and end coordinates
            s = [max(si + trim, 0) for si in start]
            e = [
                min(si + pi, di)
                for si, pi, di in zip(s, patch.shape[1:], img.shape[2:])
            ]

            # Create slices
            pred_slices = tuple(slice(si, ei) for si, ei in zip(s, e))
            patch_slices = tuple(slice(0, ei - si) for si, ei in zip(s, e))

            pred_slices_full = (slice(None),) + pred_slices
            patch_slices_full = (slice(None),) + patch_slices

            # Add patch prediction to result
            accum_pred[pred_slices_full] += patch[patch_slices_full]
            accum_wgt[pred_slices] += 1

        pbar.update(len(starts)) if verbose else None

    # Postprocess prediction
    np.divide(
        accum_pred,
        accum_wgt,
        out=accum_pred,
        where=accum_wgt != 0,
    )
    return accum_pred if affinity_mode else accum_pred[0]


def _predict_batch(img, model, starts, patch_shape, trim=8):
    """
    Extracts a batch of 3D patches from an image, runs them through the model,
    and returns the predictions.

    Parameters
    ----------
    img : numpy.ndarray
        Input image with shape (1, 1, D, H, W).
    model : torch.nn.Module
        PyTorch model used for prediction.
    starts : List[Tuple[int]]
        List of starting coordinates (z, y, x) for each image patch.
    patch_shape : Tuple[int]
        Shape of 3D patch expected by the model.
    trim : int, optional
        Number of voxels to trim from the edges of each patch in the output.
        Default is 8.

    Returns
    -------
    outputs : numpy.ndarray
        Array of model predictions for the batch with trimmed voxels set to
        zero.
    """
    # Run model
    device = next(model.parameters()).device
    inputs = _get_batch_inputs(img, starts, patch_shape, device)
    with torch.no_grad():
        outputs = torch.sigmoid(model(inputs)).cpu().numpy()

    # Trim patch boundary (if applicable)
    if trim > 0:
        outputs = outputs[..., trim:-trim, trim:-trim, trim:-trim]
    return outputs


def _get_batch_inputs(img, starts, patch_shape, device):
    """
    Extracts a batch of 3D patches from an image, pads them to a uniform size,
    and converts them to a PyTorch tensor on the specified device.

    Parameters
    ----------
    img : numpy.ndarray
        Input image with shape (1, 1, D, H, W).
    starts : List[Tuple[int]]
        List of starting coordinates (z, y, x) for each image patch.
    patch_shape : Tuple[int]
        Shape of the 3D patch expected by the model.
    device : torch.device
        Target device (CPU or GPU) where the tensor should reside.

    Returns
    -------
    inputs : torch.Tensor
        Tensor of shape containing the padded image patches, located on the
        specified device.
    """
    # Extract input patches
    inputs = np.zeros((len(starts), 1,) + patch_shape)
    for i, start in enumerate(starts):
        s = img_util.get_patch_slices(start, patch_shape, img.shape[2:])
        patch = img[(0, 0, *s)]
        inputs[i, 0, ...] = img_util.add_padding(patch, patch_shape)

    # Convert to tensor and move to device
    inputs = to_tensor(inputs, device=device)
    return inputs


# --- Segmentation and Skeletonization ---
def affinities_to_segmentation(
    affinities,
    agglomeration_thresholds=[0.6, 0.8, 0.9],
    min_segment_size=1000
):
    """
    Converts affinity maps into a segmentation using agglomerative watershed
    and filters small segments.

    Parameters
    ----------
    affinities : numpy.ndarray
        Affinity map with shape (3, D, H, W). where each channel encodes voxel
        affinities along a spatial axis.
    agglomeration_thresholds : List[float], optional
        List of merge thresholds passed to Waterz. Final segmentation is taken
        from the last threshold in the list. Default is [0.6, 0.8, 0.9].
    min_segment_size : int, optional
        Minimum size (in voxels) of segments that are kept. Default is 1000.

    Returns
    -------
    segmentation : numpy.ndarray
        A segmentation corresponding to the final agglomeration produced by
        Waterz with small segments filtered out.
    """
    # Generate segmentation
    affinities = affinities.astype(np.float32)
    segmentations = waterz.agglomerate(
        affinities,
        agglomeration_thresholds,
        aff_threshold_low=0.1,
        aff_threshold_high=0.9999,
    )

    # Postprocess segmentation
    segmentation = list(segmentations)[-1]
    segmentation = img_util.remove_small_segments(
        segmentation, min_segment_size
    )
    return segmentation


def segmentation_to_zipped_swcs(segmentation, zip_path):
    """
    Converts a labeled segmentation volume into skeletons and saves them as
    SWC files in a ZIP archive.

    Parameters
    ----------
    segmentation : numpy.ndarray
        3D array of integer labels representing a segmentation volume. Each
        unique nonzero integer corresponds to a segment.
    zip_path : str
        Path to the output ZIP archive where SWC files will be saved.
    """
    skeleton_dict = skeletonize(segmentation)
    skeletons_to_zipped_swcs(skeleton_dict, zip_path)


def skeletonize(segmentation):
    """
    Computes skeleton of the given segmentation by using the TESEAR algorithm
    implementation in Kimimaro by the SeungLab.

    Parameters
    ----------
    segmentation : numpy.ndarray
        Segmentation to be skeletonized.

    Returns
    -------
    skeleton_dict : Dict[int, osteoid.skeleton.Skeleton]
        Dictionary that maps segment IDs to skeletons.
    """
    skeleton_dict = kimimaro.skeletonize(
        segmentation,
        teasar_params={
            'scale': 1.25,
            'const': 450,
            'pdrf_exponent': 4,
            'pdrf_scale': 100000,
            'soma_detection_threshold': 1000,
            'soma_acceptance_threshold': 3500,
            'soma_invalidation_scale': 1.0,
            'soma_invalidation_const': 300,
            'max_paths': None,
          },
        anisotropy=(1.0, 1.0, 1.0),
        fix_borders=True,
        fill_holes=True,
        parallel=1,
        progress=False,
    )
    return skeleton_dict


def skeletons_to_zipped_swcs(skeleton_dict, zip_path):
    """
    Save a collection of skeletons as SWC files inside a ZIP archive.

    Parameters
    ----------
    skeleton_dict : Dict[int, osteoid.skeleton.Skeleton]
        Dictionary mapping segment IDs (int) to skeleton objects.
    zip_path : str
        Path to the output ZIP archive. If the file exists, it will be
        overwritten.
    """
    with zipfile.ZipFile(zip_path, "w") as zip_writer:
        for segment_id, skeleton in skeleton_dict.items():
            filename = f"{segment_id}.swc"
            swc_content = skeleton.to_swc()
            zip_writer.writestr(filename, swc_content)


def voxelize_skeletons(skeleton_dict, img_shape):
    """
    Converts a dictionary of skeletons into a labeled 3D voxel volume.

    Parameters
    ----------
    skeleton_dict : Dict[int, osteoid.skeleton.Skeleton]
        Dictionary mapping segment IDs (int) to skeleton objects. Each
        skeleton must have a "vertices" attribute that is an array-like
        of shape (N, 3) representing the 3D coordinates of N points.
    img_shape : Tuple[int]
        Shape of the output 3D volume (D, H, W).

    Returns
    -------
    img : numpy.ndarray
        3D integer array of shape "img_shape" where voxels occupied by
        skeletons are labeled with their segment ID.
    """
    img = np.zeros(img_shape, dtype=int)
    for segment_id, skeleton in skeleton_dict.items():
        voxels = skeleton.vertices.astype(int)
        img[tuple(voxels.T)] = segment_id
    return img


# --- Helpers ---
def count_patches(img_shape, patch_shape, overlap):
    """
    Counts the number of patches within a 3D image for a given patch shape
    and overlap between the patches.

    Parameters
    ----------
    img : ArrayLike
        Input image tensor with shape (batch, channels, depth, height, width).
    patch_shape : Tuple[int], optional
        Shape of the 3D patch expected by the model.
    overlap : Tuple[int], optional
        Number of voxels in overlap between patches along each dimension.

    Returns
    -------
    int
        Number of patches within a 3D image for a given patch shape and
        overlap between patches.
    """
    assert len(img_shape) == 5, "Image must have shape (1, 1, D, H, W)"
    stride = tuple(ps - ov for ps, ov in zip(patch_shape, overlap))
    d_range = range(0, img_shape[2] - patch_shape[0] + stride[0], stride[0])
    h_range = range(0, img_shape[3] - patch_shape[1] + stride[1], stride[1])
    w_range = range(0, img_shape[4] - patch_shape[2] + stride[2], stride[2])
    return len(d_range) * len(h_range) * len(w_range)


def generate_patch_starts(img_shape, patch_shape, overlap):
    """
    Generates starting coordinates for 3D patches extracted from an image
    tensor, based on specified patch size and overlap.

    Parameters
    ----------
    img : numpy.ndarray
        Shape of the input image.
    patch_shape : Tuple[int]
        Shape of the 3D patch expected by the model.
    overlap : Tuple[int]
        Number of voxels in overlap between patches along each dimension.

    Returns
    -------
    Iterator[Tuple[int]]
        Generates starting coordinates for image patches.
    """
    # Compute the range of starting voxel coordinate along each dimension
    assert len(img_shape) == 5, "Image must have shape (1, 1, D, H, W)"
    stride = tuple(ps - o for ps, o in zip(patch_shape, overlap))
    ranges = [
        range(0, d - ps + s, s)
        for d, ps, s in zip(img_shape[2:], patch_shape, stride)
    ]

    # Generate all starting voxel coordinates
    for start in itertools.product(*ranges):
        yield start


def load_model(path, affinity_mode=True, device="cuda"):
    """
    Loads a pretrained UNet model from a file.

    Parameters
    ----------
    path : str
        Path to the saved model weights (e.g., .pt or .pth file).
    affinity_mode : bool, optional
        If True, the model predicts affinities; if False, it predicts
        foreground–background. Default is True.
    device : str, optional
        Device to load the model onto. Default is "cuda".

    Returns
    -------
    model : torch.nn.Module
        UNet model loaded with weights and set to evaluation mode.
    """
    output_channels = 3 if affinity_mode else 1
    model = UNet3D(output_channels=output_channels)
    model.load_state_dict(torch.load(path, map_location=device))
    model.to(device)
    model.eval()
    return model


def to_tensor(arr, device="cuda"):
    """
    Converts a NumPy array containing to a PyTorch tensor and moves it to a
    GPU device.

    Parameters
    ----------
    arr : numpy.ndarray
        Array to be converted.
    device : str, optional
        Device to move array to. Default is "cuda".

    Returns
    -------
    torch.Tensor
        Tensor on GPU device.
    """
    while (len(arr.shape)) < 5:
        arr = arr[:, np.newaxis, ...]
    return torch.tensor(arr).to(device, dtype=torch.float)
